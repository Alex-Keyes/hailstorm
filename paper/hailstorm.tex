\documentclass[10pt,nocopyrightspace]{sigplanconf}

% TODO: 10pt, 11pt or nothing (default: 9pt)

\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\lstset{%
  basicstyle=\ttfamily\footnotesize,
  language=Haskell,
  frame=trBL,
  breaklines=true,
}

\title{Hailstorm: Distributed Stream Processing with Exactly Once Semantics}
\subtitle{CS240h Final Project, Spring 2014}

\authorinfo{Thomas Dimson \and Milind Ganjoo}
           {Stanford University}
           {tdimson@cs.stanford.edu \and mganjoo@cs.stanford.edu}

\maketitle

\begin{abstract}

In recent years, \textit{stream processing} has emerged as data analysis
technique to handle real-time applications where the latency of Hadoop is
inappropriate. Many popular systems, such as Twitter's Storm, provide a rigid
platform for performing distributed computations over the network. Storm-like
systems typically provide at-least-once processing with state management left
to the implementor. We present a novel distributed stream processing framework,
\textit{Hailstorm}\footnote{\url{https://github.com/hailstorm-hs/hailstorm}},
which provides a platform to perform distributed computation on streams of data
in Haskell. By restricting the class of computation to commutative monoids, our
system is able to provide exactly-once semantics with little performance loss
or added complexity.

\end{abstract}

\section{Introduction}
\label{sec:introduction}

As the Internet has evolved so have user expectations in regards to latency.
In one example, Twitter's trending topics feature allows users to see breaking
stories within minutes of their emergence. In another, Google Analytics,
administrators are able to see detailed demographic information of surfers in
real time.  The volume and velocity of the data in these systems presents
challenges to typical single-machine programs: data does not fit into memory,
and latency requirements imply that error recovery has to be automatic and
nearly instantaneous.

Like MapReduce~\cite{mapreduce} and batch processing, frameworks such as
Twitter's Storm~\cite{storm} and LinkedIn's Samza~\cite{samza} have been
created to ease the development of stream processing applications. In these
systems, events of interest are pushed into distributed queues from user-facing
applications (e.g., Twitter's web site).  As the events are popped off the
queues, the stream processing framework takes over and transforms the event
using a sequence of computations. For example, we might receive Tweets from the
queue, split on whitespace and perform a windowed count to determine topics
that are currently trending. Similar to MapReduce, developers using these
systems write algorithms that operate on individual stream \textit{units} and
emit zero or more \textit{messages} to be handled by the next stage in
computation. The frameworks distribute the events to clusters running the
computation, abstracting away the unreliable nature of the network.

This paper introduces Hailstorm, a stream processing framework in Haskell.
Unlike Storm and Samza, Hailstorm mandates that all streaming computations must
be both commutative and monoidic. Like Samza, it requires that all events must
be initially stored as messages in Apache Kafka~\cite{kafka}.  These
restrictions allow Hailstorm to make stronger processing guarantees about
events: namely, that the each event will be processed \textit{exactly} once in
the system. Furthermore, unlike Storm and Samza, state recovery under error
conditions is built-in to the framework. We utilize Haskell's purity to
guarantee that side-effects of computation are isolated to a single
\textit{sink} processor at the end of the computation sequence.


\section{Related Work}
Hailstorm's technical design is based on that of Apache Storm~\cite{storm}.
Storm is a widely used stream processing framework for
the Java Virtual Machine (JVM) allowing developers to upload jobs for continuous
processing on a Storm cluster. Developers create a directed acyclic graph of
interconnected processing layers called a \textit{topology}.
Messages are passed between layers as
\textit{tuples}. Tuples originate in a \textit{spout}, which typically reads
off of a distributed queue and are passed between layers of \textit{bolts} which
perform computation. Each bolt receives a tuple, performs a computation, and
emits zero or more tuples to the next layer. Unlike Hailstorm, the bolts have
may have side effects to their computation and state management / error recovery
is left up to each developer. Accordingly, the system is only able to provide
``at least once'' guarantees for processing each message in the queue. On
component failure, Storm enters a ``tuple replay'' state where it re-sends
messages from spouts in a topology.

The theoretical underpinings of Hailstorm are inspired by a online essay,
``Exactly Once Semantics''~\cite{jackson2014}. Jackson, a contributor to the
Storm framework, describes Kafka log offsets as a vector clock for the system
state. This clock allows separate processors to perform synchronized snapshots
without locking or direct communication. We further describe the offset clock
in Section~\ref{sec:clock}.

Google's MillWheel system~\cite{millwheel} also addresses the issue of exactly
once delivery of messages in a stream processing context. Like Storm,
messages flow through layers of computation to end up at a final result.
MillWheel provides exactly-once semantics by maintaining set of recently
processed tuples, discarding those that have recently appeared. Users of
MillWheel are required to manually ensure that all computations are
idempotent, as system failure induces message re-delivery to the same processor.

\section{Hailstorm Overview}


\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/architecture.pdf}
\caption{An example Hailstorm topology for word counts}
\label{fig:topology}
\end{figure}

Figure~\ref{fig:topology} shows a complete example of a Hailstorm system used
to calculate trending hashtags in real-time using the Twitter firehose. We give
a brief overview of the various components and describe them in detail in the
upcoming sections.

\begin{itemize}
\item \textit{Apache Kafka} is used as the sole queuing mechanism for
  messages. Messages are consumed off of Kafka \textit{partitions} and then
  entered into Hailstorm along with their \textit{offset} within the
  partition.

\item \textit{Spouts} are responsible for getting data into Hailstorm. Along
  with a user-specified conversion function, they consume \lstinline{ByteStrings}
  from Kafka and forward them as tuples to the next layer of computation.

\item \textit{Bolts} are the fundamental units of computation in Hailstorm.
  Bolts take a user-specified pure monoidic operation which takes a (state, input-tuple)
  pair and produces a (state, output-tuple) pair. Bolt state is periodically
  persisted to the snapshot store. Figure~\ref{fig:topology} shows multiple
  layers of bolts.

\item \textit{Sinks} are the final stage of Hailstorm processing. Like bolts,
  sinks take tuples from the previous layer and perform user-specified computation.
  However, unlike bolts, the computation runs inside the IO monad allowing the
  user to connect Hailstorm to the real world: databases, web services or even the
  console.

\item \textit{Topologies} are user-specified directed acyclic graphs which
  describe how bolts, spouts and sinks connect together.

\item \textit{Apache Zookeeper} is used as a global service registry for
  Hailstorm. Processors are registered as into Zookeeper and removed whenever
  failures occur.

\item \textit{The negotiator} in Hailstorm manages the state of a topology: it is
  responsible for negotiating tuple snapshots and performing error recovery. The
  negotiator itself maintains no state: if it dies, it can be resumed on any
  machine with no data loss.
\end{itemize}

\subsection{Apache Kafka}
\begin{figure}
  \includegraphics[width=0.45\textwidth]{images/kafka_log.png}
  \caption{Structure of Kafka partitions}\label{fig:kafka}
\end{figure}
Apache Kafka~\cite{kafka} is a distributed commit log that is used to buffer data between
producing systems (e.g., the Twitter Firehose) and Hailstorm topologies. As
described in~\ref{sec:clock}, Hailstorm requires the use of Kafka so it can
guarantee exactly-once processing of messaages. Messages are committed to
specific \textit{topics}, each with many \textit{partitions}. Within each
partition, commits are guaranteed to be linearly ordered according to time, with
Kafka providing an offset for each message. Figure~\ref{fig:kafka}, from the
Kafka website, shows the anatomy of a single topic as writes get fanned out to
different partitions. Consumers, such as Hailstorm spouts, are able to read from
individual partitions with a topic and consume mesages. Since the messages are
structured in a log, it is possible to ``rewind'' consumers and have them read
messages from earlier points in the log. Kafka also has a configerable replication
mechanism and is able to maintain its ordering even in the event of machine failure.

The latest version of the Kafka protocol lacks bindings for Haskell, however the
librdkafka~\cite{rdkafka} library provides up-to-date bindings for C. As part
of completing Hailstorm, we created the \textit{Haskakafka}\footnote{%
\url{http://hackage.haskell.org/package/haskakafka}} library that exposes
librdkafka through Haskell's C FFI\@. We have since made the bindings available on
Hackage for others to use.


\subsection{Apache Zookeeper}
\label{sec:zookeeper}
Apache Zookeeper~\cite{zookeeper} is a highly-available distributed
configuration service, which Hailstorm uses extensively for process
registry and synchronization. Zookeeper's data model is roughly analogous
to a tree-structured file system, where \textit{nodes} can either be
directories or small files. Nodes are either created indefinitely
or registered as \textit{ephemeral nodes}, which are automatically
deleted when their creator's connection is terminated. Most Zookeeper
libraries also implement \textit{watchers} on nodes which allow a program to be
asynchronously notified whenever a node or a node's data changes.

When a Hailstorm processor starts up, it immediately registers an ephemeral node
with its identifier underneath a Zookeeper directory called
\lstinline{living_processors}. As described in Section~\ref{sec:negotiator}, the
negotiator monitors this directory to ascertain the health of the system.

Hailstorm uses the hzk library~\cite{hzk}, which exposes the
Zookeeper C library to Haskell. The watcher notifications in Hailstorm occur in
a seperate (OS) thread, which communicates the value of the change
back to the worker thread using an MVar\footnote{See \texttt{ZKCluster.hs}}.

\subsection{Spouts}

Spouts are the starting point for any flow of information through the Hailstorm
system. Each spout has a one-to-one connection with a Kafka partition.  When
specifying the topology, a client provides a pure function that converts a
Kafka message (in the form of a byte string) to a tuple in a suitable form for
processing by downstream processors. For example, a simple ``word count''
topology could convert a word from Kafka into a \textit{(word, 1)} tuple to
facilitate counting in downstream bolts.

% TODO: code for partition -> tuple here, or in implementation details?
% I mean the word -> (word, 1) code

\subsubsection{Clock}
\label{sec:clock}

% TODO: I don't like the placement here, but this sets up the concept of a
% clock for later so it's important that it precedes all the remaining
% sections.

For the following sections, it is important to define a \textit{clock}. Put
simply, a a clock is a map of Kafka partition names to offsets within
respective partitions.  More formally, a clock $C$ will contain an offset
$C[p]$ for each Kafka partition $p$ that feeds the topology.

Hailstorm uses clocks to coordinate between processors, as outlined in
the following sections.

\subsection{Bolts}
\label{sec:bolts}

Bolts form the computational portion of the Hailstorm topology. Each bolt
maintains an internal state (represented by the type \lstinline{BoltState}),
which is updated as the computation advances. The key characteristic of
Hailstorm bolt states is that they are commutative monoids\footnote{This is not
  enforced. It is the client's responsibility to provide a \lstinline{mergeFn}
  definition that performs a commutative \lstinline{mappend} between two
  \lstinline{BoltState} instances.}.
This allows any state to be represented as a \lstinline{mappend} of one or more
older states. This has important implications for crash recoverability:
having a snapshot of an older state means that computations can be replayed
from that point forward instead of having to start from scratch.

A subset of the client-provided bolt interface is shown in
Listing~\ref{lst:bolt-defn}. Incoming tuples are converted to a monoidal
\lstinline{BoltState} to facilitate merging with the existing state. The
conversion is performed using \lstinline{tupleToStateConverter} (a function
that converts a key-value pair into a singleton map would be an example). The
\lstinline{mergeFn} performs commutative \lstinline{mappend}. The tuples
themselves are transformed using the new state and
\lstinline{transformTupleFn}) and forwarded downstream.

\begin{lstlisting}[caption=Client interface for a
bolt,label=lst:bolt-defn,float]
data Bolt =
  Bolt { -- ...
       , transformTupleFn :: PayloadTuple -> BoltState -> PayloadTuple
       , emptyState :: BoltState
       , mergeFn :: BoltState -> BoltState -> BoltState
       , tupleToStateConverter :: PayloadTuple -> BoltState
       -- ...
       }
\end{lstlisting}

\subsubsection{Updating state with incoming tuples}

Incoming tuples are always merged into the existing bolt state. Depending on
whether the negotiator has posted a desired snapshot clock (see
Section~\ref{sec:negotiator}), this could involve either one or two states:

\begin{itemize}

  \item When there is no snapshot being requested, incoming tuples are merged
  into a single state instance. The output tuple from a bolt is calculated
  using this single state.

  \item When a bolt receives a negotiator request to perform a snapshot, it
  splits its current state into state $A$ and $B$: its current state becomes
  $A$ and it initializes $B$ to \lstinline{mempty}.

  \item As long as the snapshot has not been performed, the bolt maintains two
  states---pre-snapshot state $A$ and post-snapshot state $B$---and merges
  tuples into the appropriate state based on their source partition offsets,
  and calculates the downstream tuple based on a combination of these two
  states.  More formally, for a desired snapshot clock $C$, if an incoming
  tuple originated from partition $p$ at offset $o$, then the tuple will be
  merged into state $A$ if $o \leq C[p]$; otherwise, it is merged into $B$. The
  downstream output tuple will be calculated from $A + B$.

  \item When it is time to actually persist a snapshot, the bolt makes a copy
  of pre-snapshot state $A$ and forks a thread to perform the write. In the
  main thread, it merges states $A$ and $B$ and merges new tuples into this
  single state, reverting to the no-snapshot phase.

\end{itemize}

\subsubsection{Low Water Mark}
%% TODO: Milind, you should probably talk about this

\subsubsection{Snapshot Store}
\label{sec:snapshot}
%% TODO: Milind, you should probably talk about this

\subsection{Sinks}
%% TODO: Milind, you should probably talk about this

\subsection{Negotiator}
\label{sec:negotiator}
\begin{figure}
  \includegraphics[width=0.45\textwidth]{images/master_state.pdf}
  \caption{Master state machine for a topology. After initialization, the
  topology loops between flowing and snapshotting
  indefinitely}\label{fig:masterstate}
\end{figure}

The \textit{negotiator}\footnote{See \texttt{Negotiator.hs}} has full control
over all the processors of a Hailstorm topology. The negotiator shares
two related roles:
\begin{enumerate}
  \item Forcing bolts to snapshot with a valid clock
  \item Recovering the state of the system if a processor
        becomes unreachable
\end{enumerate}

Upon registration, the negotiator creates a special ephemeral node for the
topology called the \textit{master state}. The negotiator transitions the master
state through the deterministic finite automaton shown in
Figure~\ref{fig:masterstate}. Each processor in the topology creates a
watcher for the master state, responding quickly to transitions.

Initially, the topology begins in the \lstinline{NegotiatorDisconnected} state which
is indicated by the absence of a master state node in Zookeeper. When the
negotiator boots up, it sets the master state to \lstinline{Initialize} and waits
for all processors to register under living processors (see
Section~\ref{sec:zookeeper}). Once the expected nodes have been created, the
negotiator waits for the bolts to load their snapshots from the snapshot store
(see Section~\ref{sec:snapshot}). The bolts communicate their snapshot clock
their living processor nodes, and negotiator is then able change the master state to
\lstinline{SpoutsRewind}. The spouts rewind to the clock, and pause, writing their
position into their corresponding living processor nodes. Finally, the
negotiator begins the main \textit{run loop}.

Hailstorm's run loop consists of the negotiator alternating between snapshots
and a grace period of data flow. When data is flowing without a desired snapshot
clock, the negotiator sets the master state to \lstinline{Flowing Nothing} and
then waits a configerable grace period. When the grace period expires, the
negotiator sets the state to \lstinline{SpoutsPaused} and determines the next
snapshot clock from the current spout offsets. When complete, the negotiator sets
the master state to \lstinline{Flowing NextClock}, which the bolts use to determine
their snapshot. After the bolt snapshots are complete, the negotiator returns to
\lstinline{Flowing Nothing} and loops.

The run loop can be interrupted by an unreachable processor. When a processor
becomes unresponsible, their Zookeeper connection terminates and their
corresponding ephemeral node is removed from the living processors directory.
The negotiator is asynchronously notified through a watcher, and then sets the
master state to \lstinline{Initialize}. As part of this transition, the negotiator
removes the living processor nodes for all processors in the topology. Each
processor reads this as a signal to restart, so that the topology can restart in
a clean state. Thus the \lstinline{Initialize} master state is identical to that of
the negotiator's initial start.

\section{Implementation Details}
\subsection{Command Line Interface}
%% TODO: write this

\subsection{Network Processing}
Hailstorm utilizes the Haskell Pipes~\cite{pipes} library
in place of Lazy I/O. Within
a processor, the next layer is modeled as a \lstinline{Pool} consumer%
\footnote{See \texttt{Processor/Pool.hs}} that keeps a connection pool
of downstream processor sockets. The \lstinline{Pool} consumer waits for a
Hailstorm \lstinline{Payload}, hashes it, and then sends it via a network socket
\lstinline{Handle}. Handles themselves are lazily created and maintained within a
connection pool.

Bolt and spout layers listen for incoming connections and process their
messages. After initialization, they instantiate a \textit{mailbox} using the
Pipes-Concurrency~\cite{pipes-concurrency} library and fork a listener
thread\footnote{See \texttt{Processor/Downstream.hs}}. The listener thread
accepts upstream connections and forks handlers that push incoming tuples
into the mailbox. The main processor thread creates a Pipes pipeline that
consumes messages from the mailbox, processes them in a pipes
and then sends output to a consumer (\lstinline{Pool} for bolts,
IO for sinks).

\subsection{Example Topology}
%%% TODO: find a home for this
Talk in detail about how hashtag counting works\footnote{See
\texttt{Sample/WordCountSample.hs}}

\section{Next Steps}
Our Hailstorm implementation is functionally complete, but could use some
polishing before a public release.

Currently, the only the \lstinline{HardcodedTopology} data type conforms to the the
\lstinline{Topology} type-class. Accordingly, a user of Hailstorm has to program
the network port and address for each processor into the Hailstorm binary
itself. A modification to Hailstorm would register each the network address
in the Zookeeper processor registry. The modification would allow processors to
be resumed on different machines then they started on.

In that direction, we would like to structure the framework closer to a Hadoop
cluster wherein developers upload \textit{jobs} to Hailstorm. We envision
developers ``uploading'' their processors into Zookeeper, with Hailstorm
executing the specification. The framework would utilize a package like
hint~\cite{hint} to provide dynamic code execution from Zookeeper.

\section{Conclusion}
% TODO: make this better
This paper introduces Hailstorm, a Storm-like distributed stream processing
framework for Haskell. By restricting our class of computation to commutative
monoids and by exploiting Haskell's purity, the system guarantees exactly-once
processing of messages without performance loss. Hailstorm maintains these
guarantees even in the face of machine failures and an unreliable network. We
look forward to developing it further.

\bibliography{hailstorm}{}
\bibliographystyle{abbrvnat}

\end{document}
